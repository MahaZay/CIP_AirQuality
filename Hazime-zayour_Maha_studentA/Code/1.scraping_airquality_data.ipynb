{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ebcce71-d001-4298-b919-46fd7d34930d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data from page 1\n",
      "Scraped data from page 2\n",
      "Scraped data from page 3\n",
      "Scraped data from page 4\n",
      "Scraped data from page 5\n",
      "Scraped data from page 6\n",
      "Scraped data from page 7\n",
      "Scraped data from page 8\n",
      "Data saved to '1.scraped_airquality_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# URL of the web page to scrape\n",
    "URL = 'https://discomap.eea.europa.eu/App/AQViewer/index.html?fqn=Airquality_Dissem.hra.countries_sel&EUCountries=Yes&ScenarioDescription=WHO_2021_AQG_Scen_Base&AirPollutant=PM2.5&UrbanisationDegree=All%20Areas%20(incl.unclassified)&Year=2020#'\n",
    "\n",
    "# Initialize a Chrome WebDriver and navigate to the URL\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(URL)\n",
    "wait = WebDriverWait(driver, 30)\n",
    "\n",
    "def scroll_through_table(driver, wait):\n",
    "    \"\"\"\n",
    "    Scrolls through the table on the webpage.\n",
    "    \n",
    "    This function ensures that the table is scrolled to the bottom,\n",
    "    allowing any loaded data to appear.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div#mainTable table')))\n",
    "        driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", table)\n",
    "        time.sleep(2)  # Brief pause to allow the page to load\n",
    "    except Exception as e:\n",
    "        print(\"Error while scrolling:\", e)\n",
    "\n",
    "# Function to extract table data\n",
    "column_indices = [0, 2, 3, 4, 5, 6, 8, 11]  # Define the columns to be scraped\n",
    "\n",
    "def extract_table_data(driver, wait, all_data, page_count):\n",
    "    \"\"\"\n",
    "    Extracts data from the table on the current page.\n",
    "    Each row is read, and selected columns are added to the all_data list.\n",
    "    The function also tracks the number of pages processed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div#mainTable table')))\n",
    "        rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "        for row in rows:\n",
    "            columns = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            if len(columns) > 0:  # Check if the row contains columns\n",
    "                all_data.append([columns[i].text for i in column_indices if i < len(columns)])\n",
    "        print(f\"Scraped data from page {page_count[0]}\")\n",
    "        page_count[0] += 1  # Increment page count for tracking\n",
    "    except Exception as e:\n",
    "        print(\"Error extracting table data:\", e)\n",
    "\n",
    "# Filters Setup\n",
    "year_2020_checkbox_id = \"Year_14\"  \n",
    "year_2020_checkbox = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.ID, year_2020_checkbox_id)))\n",
    "if year_2020_checkbox.is_selected():\n",
    "    year_2020_checkbox.click()  # Uncheck the year 2020\n",
    "\n",
    "years_to_select = [\"Year_12\", \"Year_13\"]  # Check the checkboxes for years 2018 and 2019\n",
    "for year_id in years_to_select:\n",
    "    year_checkbox = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.ID, year_id)))\n",
    "    if not year_checkbox.is_selected():\n",
    "        year_checkbox.click()\n",
    "    time.sleep(3)  # Wait for the page to update\n",
    "\n",
    "# Set filter for 'Air Pollutant' to include all pollutants\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"AirPollutant\")))\n",
    "driver.find_element(By.ID, \"AirPollutant\").find_element(By.XPATH, \".//option[text()='[ all ]']\").click()\n",
    "time.sleep(2)  # Wait for the filter to apply\n",
    "\n",
    "# Initialize list for storing scraped data and a list for page count\n",
    "all_data = []\n",
    "page_count = [1]\n",
    "\n",
    "# Pagination Logic\n",
    "current_page = 1\n",
    "total_pages = 8  # Total number of pages to scrape\n",
    "\n",
    "while current_page <= total_pages: # Continue looping until all pages are processed\n",
    "    try:\n",
    "        scroll_through_table(driver, wait) # Scroll through the table on the current webpage\n",
    "        extract_table_data(driver, wait, all_data, page_count) # Extract data from the table\n",
    "\n",
    "        if current_page >= total_pages:\n",
    "            break  # Break the loop if the last page is reached\n",
    "\n",
    "        page_number_selector = f\"#tableFooter > div > div.col-md-8.col-6 > nav > ul > li:nth-child({current_page + 2}) > a\"\n",
    "        page_number_element = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, page_number_selector)))\n",
    "        page_number_element.click()  # Click to navigate to the next page\n",
    "        time.sleep(3)  # Wait for the next page to load\n",
    "\n",
    "        current_page += 1  # Increment to the next page\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred on page {current_page}: {e}\")  # Print error message if any exception occurs\n",
    "        break # Break the loop on encountering an error\n",
    "\n",
    "# Close the WebDriver after finishing data extraction\n",
    "driver.quit()\n",
    "\n",
    "def save_to_csv(all_data, filename='1.scraped_airquality_data.csv'):\n",
    "    \"\"\"\n",
    "    Saves the scraped data into a CSV file.\n",
    "\n",
    "    The data is written along with a header row, defining the structure of the table.\n",
    "    \"\"\"\n",
    "    header = [\"Year\", \"Country\", \"Air Pollutant\", \"Population\", \"Populated Area [km2]\", \"Air Pollution Average [ug/m3]\",\n",
    "              \"Premature Deaths\", \"Years Of Life Lost\"]\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)  # Write the header row\n",
    "        writer.writerows(all_data)  # Write all the scraped data rows\n",
    "\n",
    "# Save the scraped data to a CSV file\n",
    "save_to_csv(all_data)\n",
    "print(\"Data saved to '1.scraped_airquality_data_stage1.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe73edf-c0ea-48af-9f9b-96cdf53a2e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006354e7-f8c7-424e-932a-ddbb45cdc6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b61a48-edae-4b83-99ca-9cae3255b59c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
